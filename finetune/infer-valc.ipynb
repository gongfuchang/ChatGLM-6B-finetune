{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code = True, load_in_8bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# peft_path = \"output/adapter_model.bin\"\n",
    "# peft_path = \"output/adapter_model_4x2.bin\"\n",
    "peft_path = \"output/adapter_model_r16.bin\"\n",
    "\n",
    "# 注意 r(attention dimension) 需要根据lora model 不同进行设置，如 8 或者 16\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32, lora_dropout=0.1 # lora scaling parameter and the dropout probability for Lora layers\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModelForCausalLM(\n",
      "      (base_model): LoraModel(\n",
      "        (model): PeftModelForCausalLM(\n",
      "          (base_model): LoraModel(\n",
      "            (model): ChatGLMForConditionalGeneration(\n",
      "              (transformer): ChatGLMModel(\n",
      "                (word_embeddings): Embedding(150528, 4096)\n",
      "                (layers): ModuleList(\n",
      "                  (0): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (1): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (2): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (3): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (4): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (5): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (6): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (7): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (8): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (9): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (10): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (11): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (12): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (13): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (14): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (15): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (16): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (17): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (18): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (19): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (20): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (21): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (22): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (23): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (24): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (25): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (26): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (27): GLMBlock(\n",
      "                    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (attention): SelfAttention(\n",
      "                      (rotary_emb): RotaryEmbedding()\n",
      "                      (query_key_value): MergedLinear8bitLt(\n",
      "                        in_features=4096, out_features=12288, bias=True\n",
      "                        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (lora_A): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                        (lora_B): Conv1d(32, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "                      )\n",
      "                      (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                    )\n",
      "                    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "                    (mlp): GLU(\n",
      "                      (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "                      (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (lm_head): Linear(in_features=4096, out_features=150528, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMTokenizer(name_or_path='THUDM/chatglm-6b', vocab_size=150344, model_max_length=2048, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# instructions = json.load(open(\"data/valc/valc_data.json\", encoding='utf-8'))\n",
    "instructions = json.load(open(\"data/valc/valc_data.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\dev\\ChatGLM-6B\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 请给出下面的网络词汇的含义\n",
      "Input: 打工人\n",
      "Answer: 打工人指的是在职场中从事体力劳动的人,通常是一些服务员、工厂工人、快递小哥等。\n",
      "### 1.Answer:\n",
      " 字面意义，给别人打工的人，调侃中带着心酸，只要不是自己当老板或是做股东，本质上都是在给别人卖命。 \n",
      "\n",
      "\n",
      "Instruction: 请给出下面的网络词汇的含义\n",
      "Input: 干饭人\n",
      "Answer: 指那些只吃饭不做事的人。\n",
      "### 2.Answer:\n",
      " 努力吃饭的人，自我调侃，没什么远大的目标，只在乎美食。 \n",
      "\n",
      "\n",
      "Instruction: 请给出下面的网络词汇的含义\n",
      "Input: 凡尔赛文学\n",
      "Answer: 指代一种以欧洲贵族社会为背景,描写贵族们的生活、爱情、婚姻、战争、政治斗争等的文学作品,因凡尔赛宫的原型就是当时的凡尔赛宫,因此这种文学也被称为“凡尔赛文学”。\n",
      "### 3.Answer:\n",
      " 通过反向表述，来刻意地“不经意”透露出自己生活优越的表达方式。 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "from convert_json import format_example\n",
    "\n",
    "def generate(input_text, temperature):\n",
    "    ids = tokenizer.encode(input_text)\n",
    "    input_ids = torch.LongTensor([ids])\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=300,\n",
    "        do_sample=False,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    out_text = tokenizer.decode(out[0])\n",
    "    return out_text\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:3]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature['context']\n",
    "        out_text = generate(input_text, 0)\n",
    "        answer = out_text.replace(input_text, \"\").replace(\"\\nEND\", \"\").strip()\n",
    "        item['infer_answer'] = answer\n",
    "        print(out_text)\n",
    "        print(f\"### {idx+1}.Instruct-Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        answers.append({'index': idx, **item})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\dev\\ChatGLM-6B\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你是谁 我是一个名为 ChatGLM-6B 的人工智能助手,是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('你是谁', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\dev\\ChatGLM-6B\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'写一句土味情话 你是我的小甜心。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('写一句土味情话', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
